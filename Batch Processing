Batch processing is a technique for handling and processing large volumes of data by dividing it into smaller, manageable chunks or “batches.” 
This method allows applications to efficiently process massive datasets, such as large files, database records, or messages,by splitting the workload into smaller pieces that can be processed independently and in parallel.
The main components include the Batch Job, Batch Step, and Batch Aggregator. 
The Batch Job component automatically breaks down large data into smaller chunks and saves them in persistent queues. 
This allows for the processing of large amounts of data. If the application crashes or is redeployed, the job can resume from where it left off, ensuring reliability.

Common use cases for batch processing include:
  1.  Synchronizing data sets between business applications, such as syncing contacts between NetSuite and Salesforce.
  2.  Extracting, transforming and loading (ETL) information into a target system, such as uploading data from a flat file (CSV) to Hadoop.
  3.  Handling large quantities of incoming data from an API to a legacy system.


BatchJob has three phases in Mule 4.

1.Load And Dispatch:  It will create job instances, convert payload into collection of records and then split the collection into individual records for processing.

2.Process:  In this phase, it processes all individual records asynchronously. Batch step allows you to filter records.
            We can also use batch aggregator processor to aggregate records into groups. 
            For example, if you want to process 10 as one group, you can set the aggregate processor size as 10.

3.On Complete: The last and optional phase gives a summary of the payload. It will give us how many records are processed, and how many failed


Batch Aggregator: A batch aggregator scope to process fixed-size groups of records inside a batch aggregator scope.
                  You can configure the batch aggregator scope to upsert, for example, 100 records at a time

Below are the configuration details we can define for Batch job:

Name –   Batch job name

Max Failed Records – 	this value define on how many failed records we should stop the Batch job processing. 
			By default 0 means if any record fail then it will stop the batch job execution. 
			-1 means not to stop even if all the incoming payload fails
Scheduling Strategy – 	Batch jobs execution

			ORDERED_SQUENTIAL (the default): If several job instances are in an executable state at the same time, the instances execute one at a time based on their creation timestamp.
			ROUND_ROBIN: This setting attempts to execute all available instances of a batch job using a round-robin algorithm to assign the available resources.
Job instance id –
			 given or provided to each batch job created for batch processing
Batch Block Size – 
			default 100 – Block size define the number of records given to each thread for execution. 
			Bigger the number less I/O operation will be performed by the thread but need more memory to hold block. 
			So always match the block size w.r.t to vCore size (thread count) and input payload. 
			This directly effect the performance of the batch job processing
Max Concurrency –
			 Defining the number of threads which can participate in batch processing if you don’t want the default thread count which depends
			 on vCore size of the machine
Target –  		to save the Batch stats result
